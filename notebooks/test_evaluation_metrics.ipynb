{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/aw624/Low_Rank_Generative_Models/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(os.path.abspath('..'))  # Go up one directory if notebook is in /notebooks\n",
    "from src.preprocessing import create_dataloader\n",
    "from src.config import TrainingConfig\n",
    "from src.eval import Eval\n",
    "from diffusers import DiTPipeline\n",
    "from src.vae import DummyAutoencoderKL\n",
    "from src.DiT import create_model, create_noise_scheduler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset uoft-cs/cifar10 train split with 50000 images\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = create_dataloader(\"uoft-cs/cifar10\", \"train\", TrainingConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img': tensor([[[[-0.3490, -0.3490, -0.3647,  ...,  0.2471,  0.2157,  0.2157],\n",
      "          [-0.3098, -0.3333, -0.3490,  ...,  0.2549,  0.0824,  0.1373],\n",
      "          [-0.3020, -0.3176, -0.3176,  ...,  0.1843,  0.0353,  0.1922],\n",
      "          ...,\n",
      "          [-0.3490, -0.2549, -0.2235,  ..., -0.2392, -0.2078, -0.2627],\n",
      "          [-0.2784, -0.2471, -0.2627,  ..., -0.2706, -0.2471, -0.2627],\n",
      "          [-0.2471, -0.3176, -0.2941,  ..., -0.2471, -0.2549, -0.2627]],\n",
      "\n",
      "         [[-0.1765, -0.1765, -0.1922,  ...,  0.3412,  0.2784,  0.2549],\n",
      "          [-0.1373, -0.1608, -0.1765,  ...,  0.3569,  0.1686,  0.1922],\n",
      "          [-0.1294, -0.1451, -0.1451,  ...,  0.3176,  0.1373,  0.2706],\n",
      "          ...,\n",
      "          [-0.1686, -0.0745, -0.0431,  ..., -0.0588, -0.0275, -0.0824],\n",
      "          [-0.0980, -0.0667, -0.0824,  ..., -0.0902, -0.0667, -0.0824],\n",
      "          [-0.0667, -0.1373, -0.1137,  ..., -0.0667, -0.0745, -0.0824]],\n",
      "\n",
      "         [[-0.3412, -0.3412, -0.3647,  ...,  0.2863,  0.2392,  0.2157],\n",
      "          [-0.3020, -0.3255, -0.3412,  ...,  0.2863,  0.0980,  0.1294],\n",
      "          [-0.2941, -0.3098, -0.3176,  ...,  0.2157,  0.0431,  0.1843],\n",
      "          ...,\n",
      "          [-0.3725, -0.2784, -0.2549,  ..., -0.2863, -0.2549, -0.3098],\n",
      "          [-0.3020, -0.2706, -0.2863,  ..., -0.3020, -0.2784, -0.2941],\n",
      "          [-0.2706, -0.3412, -0.3176,  ..., -0.2706, -0.2784, -0.2863]]],\n",
      "\n",
      "\n",
      "        [[[-0.1451, -0.1137, -0.0588,  ..., -0.0824, -0.0824, -0.0824],\n",
      "          [-0.1216, -0.0980, -0.0588,  ..., -0.0510, -0.0588, -0.0510],\n",
      "          [-0.1059, -0.1059, -0.0745,  ..., -0.0510, -0.0510, -0.0510],\n",
      "          ...,\n",
      "          [-0.4588, -0.4510, -0.4353,  ..., -0.4039, -0.4039, -0.3804],\n",
      "          [-0.4353, -0.4353, -0.4039,  ..., -0.3961, -0.4118, -0.4039],\n",
      "          [-0.4196, -0.4353, -0.4196,  ..., -0.4275, -0.4196, -0.4196]],\n",
      "\n",
      "         [[ 0.0510,  0.0588,  0.0902,  ...,  0.0980,  0.0980,  0.0980],\n",
      "          [ 0.0667,  0.0745,  0.0980,  ...,  0.1137,  0.1137,  0.1137],\n",
      "          [ 0.0588,  0.0588,  0.0902,  ...,  0.1059,  0.1059,  0.1059],\n",
      "          ...,\n",
      "          [-0.2314, -0.2314, -0.2157,  ..., -0.1843, -0.1843, -0.1686],\n",
      "          [-0.2078, -0.2078, -0.1765,  ..., -0.1765, -0.1922, -0.1922],\n",
      "          [-0.1922, -0.2078, -0.1922,  ..., -0.2157, -0.2000, -0.2157]],\n",
      "\n",
      "         [[ 0.2078,  0.2000,  0.2078,  ...,  0.3255,  0.3255,  0.3255],\n",
      "          [ 0.2157,  0.2078,  0.2235,  ...,  0.3412,  0.3333,  0.3333],\n",
      "          [ 0.2000,  0.1922,  0.2235,  ...,  0.3098,  0.3098,  0.3098],\n",
      "          ...,\n",
      "          [-0.1686, -0.1686, -0.1529,  ..., -0.0745, -0.0745, -0.0667],\n",
      "          [-0.1451, -0.1451, -0.1137,  ..., -0.0667, -0.0824, -0.0824],\n",
      "          [-0.1373, -0.1451, -0.1294,  ..., -0.1059, -0.0902, -0.1059]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0745,  0.0745,  0.0824,  ..., -0.5922, -0.7098, -0.7412],\n",
      "          [ 0.0510,  0.0588,  0.0667,  ..., -0.6235, -0.7412, -0.7725],\n",
      "          [ 0.0353,  0.0510,  0.0667,  ..., -0.7176, -0.7961, -0.8196],\n",
      "          ...,\n",
      "          [ 0.1686, -0.2784, -0.6627,  ...,  0.5686,  0.4275,  0.2706],\n",
      "          [ 0.2706, -0.0275, -0.5059,  ...,  0.7020,  0.6471,  0.5529],\n",
      "          [ 0.3647,  0.1216, -0.4745,  ...,  0.7176,  0.7333,  0.7569]],\n",
      "\n",
      "         [[ 0.1765,  0.1843,  0.1922,  ..., -0.4118, -0.5843, -0.6392],\n",
      "          [ 0.1529,  0.1686,  0.1686,  ..., -0.4667, -0.6471, -0.7176],\n",
      "          [ 0.1373,  0.1608,  0.1765,  ..., -0.6235, -0.7569, -0.7882],\n",
      "          ...,\n",
      "          [ 0.0902, -0.3647, -0.6941,  ...,  0.5686,  0.4510,  0.3176],\n",
      "          [ 0.2157, -0.0824, -0.5294,  ...,  0.6863,  0.6471,  0.5608],\n",
      "          [ 0.3412,  0.0980, -0.4745,  ...,  0.7098,  0.7255,  0.7490]],\n",
      "\n",
      "         [[ 0.2863,  0.2627,  0.2627,  ..., -0.3333, -0.5373, -0.6078],\n",
      "          [ 0.2627,  0.2471,  0.2392,  ..., -0.3804, -0.5922, -0.6706],\n",
      "          [ 0.2549,  0.2392,  0.2471,  ..., -0.5137, -0.6784, -0.7333],\n",
      "          ...,\n",
      "          [-0.0588, -0.4196, -0.6314,  ...,  0.7176,  0.6078,  0.4667],\n",
      "          [ 0.1294, -0.1216, -0.4510,  ...,  0.8118,  0.7882,  0.7176],\n",
      "          [ 0.3333,  0.0902, -0.4039,  ...,  0.8275,  0.8431,  0.8745]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1137,  0.3255,  0.2314,  ..., -0.1765, -0.1922, -0.2078],\n",
      "          [-0.3098,  0.1686,  0.4196,  ..., -0.2392, -0.2314, -0.2392],\n",
      "          [-0.4667, -0.1216,  0.3725,  ..., -0.2471, -0.2549, -0.2471],\n",
      "          ...,\n",
      "          [-0.3647, -0.1922, -0.0275,  ..., -0.2549, -0.2235, -0.2784],\n",
      "          [-0.2549, -0.2549, -0.3490,  ..., -0.2078, -0.1922, -0.2157],\n",
      "          [ 0.1843,  0.0980, -0.1765,  ..., -0.1765, -0.1843, -0.1765]],\n",
      "\n",
      "         [[-0.2000,  0.1373,  0.1922,  ...,  0.2235,  0.2078,  0.2000],\n",
      "          [-0.3255, -0.0039,  0.3098,  ...,  0.2235,  0.2314,  0.2157],\n",
      "          [-0.4118, -0.1373,  0.2235,  ...,  0.2078,  0.2078,  0.2000],\n",
      "          ...,\n",
      "          [-0.1529,  0.0588,  0.2392,  ...,  0.0588,  0.0902,  0.0745],\n",
      "          [-0.2235, -0.1451, -0.1529,  ...,  0.1686,  0.2000,  0.2157],\n",
      "          [ 0.1216,  0.0667, -0.0902,  ...,  0.2157,  0.2314,  0.2471]],\n",
      "\n",
      "         [[-0.2784, -0.0667, -0.0824,  ..., -0.3804, -0.3961, -0.4431],\n",
      "          [-0.2471, -0.1529, -0.0275,  ..., -0.4353, -0.4275, -0.4510],\n",
      "          [-0.1059, -0.1765, -0.0667,  ..., -0.4431, -0.4431, -0.4353],\n",
      "          ...,\n",
      "          [-0.5529, -0.4667, -0.3412,  ..., -0.4118, -0.3176, -0.3490],\n",
      "          [-0.3882, -0.4588, -0.5765,  ..., -0.4980, -0.4667, -0.4431],\n",
      "          [ 0.0902, -0.0588, -0.2784,  ..., -0.4824, -0.4980, -0.4196]]],\n",
      "\n",
      "\n",
      "        [[[-0.0431, -0.0275, -0.0039,  ...,  0.2392,  0.2157,  0.1059],\n",
      "          [ 0.1686,  0.2157,  0.2471,  ...,  0.3412,  0.3098,  0.1922],\n",
      "          [ 0.1922,  0.2235,  0.2314,  ...,  0.3569,  0.3412,  0.2235],\n",
      "          ...,\n",
      "          [ 0.1529,  0.1608,  0.0902,  ...,  0.1059,  0.1216,  0.0275],\n",
      "          [ 0.1686,  0.1686,  0.1059,  ...,  0.0745,  0.0745, -0.0275],\n",
      "          [ 0.2157,  0.2314,  0.1843,  ...,  0.0667,  0.0588, -0.0431]],\n",
      "\n",
      "         [[-0.3020, -0.2863, -0.2627,  ..., -0.0667, -0.0824, -0.1608],\n",
      "          [-0.1451, -0.1059, -0.0667,  ..., -0.0039, -0.0196, -0.1137],\n",
      "          [-0.0980, -0.0745, -0.0667,  ..., -0.0039, -0.0196, -0.1137],\n",
      "          ...,\n",
      "          [-0.1216, -0.1216, -0.1922,  ..., -0.1608, -0.1373, -0.2078],\n",
      "          [-0.1216, -0.1294, -0.1922,  ..., -0.1922, -0.1843, -0.2706],\n",
      "          [-0.1059, -0.0980, -0.1451,  ..., -0.1922, -0.2000, -0.2784]],\n",
      "\n",
      "         [[-0.4980, -0.4902, -0.4667,  ..., -0.2941, -0.3098, -0.3725],\n",
      "          [-0.3647, -0.3255, -0.2941,  ..., -0.2471, -0.2706, -0.3490],\n",
      "          [-0.3333, -0.3176, -0.3098,  ..., -0.2235, -0.2392, -0.3176],\n",
      "          ...,\n",
      "          [-0.4039, -0.4118, -0.4824,  ..., -0.3961, -0.3725, -0.4353],\n",
      "          [-0.4039, -0.4196, -0.4824,  ..., -0.4275, -0.4275, -0.4902],\n",
      "          [-0.3725, -0.3725, -0.4196,  ..., -0.4275, -0.4353, -0.5059]]],\n",
      "\n",
      "\n",
      "        [[[-0.8745, -0.8824, -0.9373,  ..., -0.4588, -0.4196,  0.5451],\n",
      "          [-0.8902, -0.8824, -0.9216,  ..., -0.4510, -0.4039,  0.5451],\n",
      "          [-0.9137, -0.9059, -0.9137,  ..., -0.4275, -0.3882,  0.5843],\n",
      "          ...,\n",
      "          [-0.0118,  0.0275,  0.0275,  ...,  0.0431,  0.0275, -0.0196],\n",
      "          [ 0.1216,  0.0588,  0.0745,  ..., -0.0039, -0.0353,  0.0275],\n",
      "          [ 0.1216,  0.0667,  0.0980,  ...,  0.0196, -0.0353,  0.0353]],\n",
      "\n",
      "         [[-0.6235, -0.6314, -0.6863,  ..., -0.5216, -0.5294,  0.4745],\n",
      "          [-0.6392, -0.6314, -0.6784,  ..., -0.5137, -0.5059,  0.4824],\n",
      "          [-0.6627, -0.6549, -0.6706,  ..., -0.4980, -0.4902,  0.5216],\n",
      "          ...,\n",
      "          [-0.0118,  0.0275,  0.0275,  ...,  0.0431,  0.0275, -0.0196],\n",
      "          [ 0.1216,  0.0588,  0.0745,  ...,  0.0039, -0.0275,  0.0353],\n",
      "          [ 0.1059,  0.0588,  0.0902,  ...,  0.0275, -0.0275,  0.0431]],\n",
      "\n",
      "         [[-0.7412, -0.7490, -0.8039,  ..., -0.6863, -0.6706,  0.2000],\n",
      "          [-0.7569, -0.7490, -0.7961,  ..., -0.6549, -0.6784,  0.2000],\n",
      "          [-0.7804, -0.7725, -0.7882,  ..., -0.6314, -0.6941,  0.2235],\n",
      "          ...,\n",
      "          [-0.0902, -0.0353, -0.0196,  ..., -0.0510, -0.0745, -0.1137],\n",
      "          [ 0.0431, -0.0196, -0.0039,  ..., -0.1059, -0.1294, -0.0745],\n",
      "          [ 0.0353, -0.0196,  0.0118,  ..., -0.0902, -0.1373, -0.0745]]]]), 'label': tensor([5, 8, 3, 1, 5, 3, 2, 3, 7, 6, 4, 1, 7, 0, 8, 6, 1, 2, 5, 7, 8, 8, 5, 9,\n",
      "        8, 8, 2, 6, 1, 3, 3, 4, 7, 7, 2, 3, 8, 6, 0, 2, 6, 4, 9, 2, 5, 8, 5, 6,\n",
      "        1, 1, 1, 1, 4, 0, 5, 1, 7, 8, 9, 7, 7, 4, 3, 9, 6, 5, 0, 4, 9, 5, 1, 2,\n",
      "        5, 2, 2, 3, 0, 0, 1, 7, 6, 9, 9, 5, 3, 0, 2, 9, 3, 1, 5, 0, 1, 3, 2, 9,\n",
      "        2, 7, 5, 1, 9, 1, 4, 2, 6, 4, 5, 2, 8, 5, 2, 6, 0, 9, 6, 9, 1, 4, 7, 1,\n",
      "        9, 1, 5, 3, 1, 6, 4, 5])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "model = create_model(TrainingConfig())\n",
    "print(TrainingConfig().image_size)\n",
    "model.load_state_dict(torch.load(Path(\"/vol/bitbucket/aw624/Low_Rank_Generative_Models/logs/DiT20250423_205135/model.pt\")))\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "pipeline = DiTPipeline(\n",
    "    transformer=model,\n",
    "    scheduler=create_noise_scheduler(TrainingConfig()),\n",
    "    vae=DummyAutoencoderKL(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/aw624/Low_Rank_Generative_Models/venv/lib/python3.12/site-packages/diffusers/pipelines/dit/pipeline_dit.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_labels = torch.tensor(class_labels, device=self._execution_device).reshape(-1)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:36<00:00, 27.04it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_num = 10000//64\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    image = pipeline(\n",
    "        class_labels=torch.zeros(64, dtype=torch.long),\n",
    "                num_inference_steps=1000,\n",
    "                output_type=\"tensor\"\n",
    "            ).images\n",
    "    \n",
    "    generated_images.extend(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1068804/1114798230.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  generated_images = torch.tensor(generated_images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "generated_images = torch.tensor(generated_images)\n",
    "\n",
    "print(generated_images.shape)\n",
    "\n",
    "generated_images = generated_images.permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_pretrained in module diffusers.pipelines.pipeline_utils:\n",
      "\n",
      "from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], **kwargs) -> Self class method of diffusers.pipelines.dit.pipeline_dit.DiTPipeline\n",
      "    Instantiate a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      "\n",
      "    The pipeline is set in evaluation mode (`model.eval()`) by default.\n",
      "\n",
      "    If you get the error message below, you need to finetune the weights for your downstream task:\n",
      "\n",
      "    ```\n",
      "    Some weights of UNet2DConditionModel were not initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n",
      "    - conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\n",
      "    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "    ```\n",
      "\n",
      "    Parameters:\n",
      "        pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      "            Can be either:\n",
      "\n",
      "                - A string, the *repo id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      "                  hosted on the Hub.\n",
      "                - A path to a *directory* (for example `./my_pipeline_directory/`) containing pipeline weights\n",
      "                  saved using\n",
      "                [`~DiffusionPipeline.save_pretrained`].\n",
      "                - A path to a *directory* (for example `./my_pipeline_directory/`) containing a dduf file\n",
      "        torch_dtype (`str` or `torch.dtype` or `dict[str, Union[str, torch.dtype]]`, *optional*):\n",
      "            Override the default `torch.dtype` and load the model with another dtype. If \"auto\" is passed, the\n",
      "            dtype is automatically derived from the model's weights. To load submodels with different dtype pass a\n",
      "            `dict` (for example `{'transformer': torch.bfloat16, 'vae': torch.float16}`). Set the default dtype for\n",
      "            unspecified components with `default` (for example `{'transformer': torch.bfloat16, 'default':\n",
      "            torch.float16}`). If a component is not specified and no default is set, `torch.float32` is used.\n",
      "        custom_pipeline (`str`, *optional*):\n",
      "\n",
      "            <Tip warning={true}>\n",
      "\n",
      "            ðŸ§ª This is an experimental feature and may change in the future.\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "            Can be either:\n",
      "\n",
      "                - A string, the *repo id* (for example `hf-internal-testing/diffusers-dummy-pipeline`) of a custom\n",
      "                  pipeline hosted on the Hub. The repository must contain a file called pipeline.py that defines\n",
      "                  the custom pipeline.\n",
      "                - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      "                  [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      "                  names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      "                  instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      "                  current main branch of GitHub.\n",
      "                - A path to a directory (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      "                  must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      "\n",
      "            For more information on how to load and create custom pipelines, please have a look at [Loading and\n",
      "            Adding Custom\n",
      "            Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)\n",
      "        force_download (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      "            cached versions if they exist.\n",
      "        cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      "            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      "            is not used.\n",
      "\n",
      "        proxies (`Dict[str, str]`, *optional*):\n",
      "            A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      "            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "        output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      "        local_files_only (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      "            won't be downloaded from the Hub.\n",
      "        token (`str` or *bool*, *optional*):\n",
      "            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      "            `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      "            allowed by Git.\n",
      "        custom_revision (`str`, *optional*):\n",
      "            The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      "            `revision` when loading a custom pipeline from the Hub. Defaults to the latest stable ðŸ¤— Diffusers\n",
      "            version.\n",
      "        mirror (`str`, *optional*):\n",
      "            Mirror source to resolve accessibility issues if youâ€™re downloading a model in China. We do not\n",
      "            guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      "            information.\n",
      "        device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n",
      "            A map that specifies where each submodule should go. It doesnâ€™t need to be defined for each\n",
      "            parameter/buffer name; once a given module name is inside, every submodule of it will be sent to the\n",
      "            same device.\n",
      "\n",
      "            Set `device_map=\"auto\"` to have ðŸ¤— Accelerate automatically compute the most optimized `device_map`. For\n",
      "            more information about each option see [designing a device\n",
      "            map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      "        max_memory (`Dict`, *optional*):\n",
      "            A dictionary device identifier for the maximum memory. Will default to the maximum memory available for\n",
      "            each GPU and the available CPU RAM if unset.\n",
      "        offload_folder (`str` or `os.PathLike`, *optional*):\n",
      "            The path to offload weights if device_map contains the value `\"disk\"`.\n",
      "        offload_state_dict (`bool`, *optional*):\n",
      "            If `True`, temporarily offloads the CPU state dict to the hard drive to avoid running out of CPU RAM if\n",
      "            the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to `True`\n",
      "            when there is some disk offload.\n",
      "        low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      "            Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
      "            tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      "            Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
      "            argument to `True` will raise an error.\n",
      "        use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      "            If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
      "            safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
      "            weights. If set to `False`, safetensors weights are not loaded.\n",
      "        use_onnx (`bool`, *optional*, defaults to `None`):\n",
      "            If set to `True`, ONNX weights will always be downloaded if present. If set to `False`, ONNX weights\n",
      "            will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class attribute which is\n",
      "            `False` for non-ONNX pipelines and `True` for ONNX pipelines. ONNX weights include both files ending\n",
      "            with `.onnx` and `.pb`.\n",
      "        kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      "            Can be used to overwrite load and saveable variables (the pipeline components of the specific pipeline\n",
      "            class). The overwritten components are passed directly to the pipelines `__init__` method. See example\n",
      "            below for more information.\n",
      "        variant (`str`, *optional*):\n",
      "            Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      "            loading `from_flax`.\n",
      "        dduf_file(`str`, *optional*):\n",
      "            Load weights from the specified dduf file.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "    To use private or [gated](https://huggingface.co/docs/hub/models-gated#gated-models) models, log-in with\n",
      "    `huggingface-cli login`.\n",
      "\n",
      "    </Tip>\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    ```py\n",
      "    >>> from diffusers import DiffusionPipeline\n",
      "\n",
      "    >>> # Download pipeline from huggingface.co and cache.\n",
      "    >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n",
      "\n",
      "    >>> # Download pipeline that requires an authorization token\n",
      "    >>> # For more information on access tokens, please refer to this section\n",
      "    >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n",
      "    >>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
      "\n",
      "    >>> # Use a different scheduler\n",
      "    >>> from diffusers import LMSDiscreteScheduler\n",
      "\n",
      "    >>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
      "    >>> pipeline.scheduler = scheduler\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DiTPipeline.from_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset uoft-cs/cifar10 test split with 10000 images\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = create_dataloader(\"uoft-cs/cifar10\", \"test\", TrainingConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([64, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    print((batch[\"img\"].dtype))\n",
    "    print(batch[\"img\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing real features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "eval = Eval(test_dataloader, eval_dataset_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/aw624/Low_Rank_Generative_Models/venv/lib/python3.12/site-packages/diffusers/pipelines/dit/pipeline_dit.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_labels = torch.tensor(class_labels, device=self._execution_device).reshape(-1)\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:37<00:00, 27.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fid': 64.75636291503906}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.compute_metrics(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fid': 60.301753997802734}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.compute_metrics(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.lib.ChunkedArray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataloader.dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
